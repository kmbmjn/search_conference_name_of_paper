The Marginal Value of Adaptive Gradient Methods in Machine Learning
Improving Generalization Performance by Switching from Adam to SGD
Deep Mutual Learning
Born-Again Neural Networks
Data Distillation: Towards Omni-Supervised Learning
Weight Standardization
Knowledge Transfer with Jacobian Matching
Paraphrasing Complex Network: Network Compression via Factor Transfer
Self-supervised Knowledge Distillation Using Singular Value Decomposition
SmoothGrad: removing noise by adding noise
From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation
THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS
